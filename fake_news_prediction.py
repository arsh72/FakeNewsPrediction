# -*- coding: utf-8 -*-
"""Fake News Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIgTWk_B93U5Rwu2e5lKeYhNCIuIcaQp
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Embedding,Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM,Bidirectional,GRU
from tensorflow.keras.layers import Dense
from sklearn.metrics import classification_report,accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import MultinomialNB
# %matplotlib inline

nltk.download('stopwords')

train_data = pd.read_csv('/content/sample_data/train.csv')
test_data =  pd.read_csv('/content/sample_data/test.csv')

print(train_data.head())

print(test_data.head())

print(train_data.shape)

print(test_data.shape)

print(train_data.describe())

print(train_data.info())

print(train_data.isnull().sum())

print(test_data.isnull().sum())

# Adding empty spaces to the Nan values in the dataset

def handle_null(train_data, test_data):
  train = train_data.fillna(" ")
  test  = test_data.fillna(" ") 
  return train,test

train_data,test_data = handle_null(train_data,test_data)

train_data['Author&Title'] = train_data['author'] + ' ' + train_data['title']
test_data['Author&Title'] = test_data['author'] + ' ' + test_data['title']

print(train_data['Author&Title'])

from wordcloud import WordCloud
from collections import Counter

# Plotting Word Cloud

# Finding the most common word

words = list(train_data['Author&Title'].apply(lambda x:x.split()))
words = [x for y in words for x in y]

most_common40 = Counter(words).most_common(40)
wc = WordCloud(width=1200, height=500,collocations=False, background_color="white",colormap="tab20b").generate(" ".join(words))

# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words
plt.figure(figsize=(25,10))
# generate word cloud, interpolation 
plt.imshow(wc, interpolation='bilinear')
_ = plt.axis("off")

X = train_data.drop(columns = 'label', axis = 1)
Y = train_data['label']

print(X)

print(Y)

print(test_data)

#Resetting the index

messages = X.copy()
messages.reset_index(inplace=True)
messages_test = test_data.copy()
messages_test.reset_index(inplace=True)

stem = PorterStemmer()
def preprocess(input):
  corpus = []
  for i in range(0,len(input)):
    data = re.sub('[^a-zA-Z]',' ',input['Author&Title'][i])
    data = data.lower()
    data = data.split()
    data = [stem.stem(word) for word in data if not word in stopwords.words('english')]
    data = ' '.join(data)
    corpus.append(data)
  return corpus

train_corpus = preprocess(messages)
test_corpus = preprocess(messages_test)

print(train_corpus[1])
print(test_corpus[1])

vocab_size = 5000
one_hot_train = [one_hot(word,vocab_size) for word in train_corpus]
one_hot_test  = [one_hot(word,vocab_size) for word in test_corpus]

# Embedding Representation 
sent_length = 20
embed_train = pad_sequences(one_hot_train,padding='pre',maxlen=sent_length)
embed_test  = pad_sequences(one_hot_test,padding='pre',maxlen=sent_length)

print(embed_train)
print(embed_test)

x_final = np.array(embed_train)
y_final = np.array(Y)
x_test_final = np.array(embed_test)

X_train, X_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.20, random_state=41)

# LOGISTIC REGRESSION
model_LR = LogisticRegression()
model_LR.fit(X_train,y_train)
predictions_LR = model_LR.predict(X_test)
classification_report_LR = classification_report(y_test,predictions_LR)
print('Classification Report\n', classification_report_LR)
print('Acuracy Score\n', accuracy_score(y_test,predictions_LR))

# NAIVE BAYES

model_NB = MultinomialNB()
model_NB.fit(X_train,y_train)
predictions_NB = model_NB.predict(X_test)
classification_report_NB = classification_report(y_test,predictions_NB)
print('Classification Report\n', classification_report_NB)
print('Acuracy Score\n', accuracy_score(y_test,predictions_NB))

# RANDOM FOREST

model_RFC = RandomForestClassifier()
model_RFC.fit(X_train,y_train)
predictions_RFC = model_RFC.predict(X_test)
classification_report_RFC = classification_report(y_test,predictions_RFC)
print('Classification Report\n', classification_report_RFC)
print('Acuracy Score\n', accuracy_score(y_test,predictions_RFC))

# XGBOOST

model_XGB = XGBClassifier()
model_XGB.fit(X_train,y_train)
predictions_XGB = model_XGB.predict(X_test)
classification_report_XGB = classification_report(y_test,predictions_XGB)
print('Classification Report\n', classification_report_XGB)
print('Acuracy Score\n', accuracy_score(y_test,predictions_XGB))

# NEURAL NETWORKS: LSTM

# Creating the LSTM Model for prediction
embedding_feature_vector = 40
model = Sequential()
model.add(Embedding(vocab_size,embedding_feature_vector,input_length=sent_length))
model.add(Dropout(0.3))
model.add(LSTM(100))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)

# # Making Predictions on test data with XGBOOST MODEL
labels = pd.read_csv('/content/sample_data/labels.csv')
target = labels['label']
predictions_test = model_XGB.predict(x_test_final)

print(labels)

error = np.mean(predictions_test != target)
print("Error rate  = ",error*100," %")